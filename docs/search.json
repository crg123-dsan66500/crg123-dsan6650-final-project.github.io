[
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "Results",
    "section": "",
    "text": "The empirical performance of all agents in Bellman’s Bakery is summarized below. The results should be read through the lens of reward shaping and capacity constraints: throughput is limited by both service speed and oven space, while the shaped reward penalizes waiting, abandonment, and excess inventory. Within this structure, agents learn to optimize the reward they are given—not necessarily profit. The Newsvendor heuristic achieves relatively high profit by baking aggressively and accepting waste as a cost of doing business. PPO, by contrast, learns a conservative, low-waste strategy that under-produces and sacrifices margin. QRDQN explores more broadly and improves profit in some runs, but its decisions still reflect the incentives baked into the reward function. Pricing focused solely on profit increases margins slightly but leaves the production strategy mostly unchanged."
  },
  {
    "objectID": "results.html#full-horizon-evaluation",
    "href": "results.html#full-horizon-evaluation",
    "title": "Results",
    "section": "Full-Horizon Evaluation",
    "text": "Full-Horizon Evaluation\nMetrics aggregate 10 independent seeds; each seed is evaluated for 20 full days (240 ticks/day) under non‑stationary demand. Daily results are averaged per seed and then across seeds to approximate long‑run behavior.\n\n\n\n\n\n\n\n\n\n\nPolicy\nNet Profit\nAbandoned\nAvg Wait (s)\nWaste\n\n\n\n\nNewsvendor (best)\n95.22\n1.5%\n12.7\n41.1%\n\n\nPPO v3 best\n35.28\n74.7%\n49.1\n14.2%\n\n\nPPO+Price v3 best\n23.36\n81.3%\n52.1\n37.7%\n\n\nPPO+Par 1e6\n-20.47\n91.1%\n56.0\n82.8%\n\n\nPPO+Price (profit-only)\n25.79\n83.5%\n53.2\n44.5%\n\n\nQRDQN 1e6\n41.32\n21.2%\n22.7\n44.9%\n\n\n\n\nInterpretation\nNewsvendor dominates by baking aggressively, tolerating waste, and sustaining throughput. In contrast, PPO gravitates toward an under-producing regime, shaped by an objective that penalizes waiting, abandonment, and leftover inventory. The resulting behavior achieves very low waste, but at a cost: chronic stockouts, high abandonment, and long waits. In short, PPO is optimal for the reward it sees—not for economic profit.\nBandit variants follow a similar pattern. Composite-metric pricing amplifies the agent’s conservative stance, while profit-only pricing pushes margins slightly higher without meaningfully shifting production behavior. PPO+Par performs worst overall, as the par mechanism locks the agent into persistent underproduction with no viable path to adjust.\nTakeaway: The RL agents are not failing. They are learning exactly what they were told to optimize—and that turns out to be the wrong thing."
  },
  {
    "objectID": "results.html#weekly-aggregation-a-check-for-robustness",
    "href": "results.html#weekly-aggregation-a-check-for-robustness",
    "title": "Results",
    "section": "Weekly Aggregation: A check for robustness",
    "text": "Weekly Aggregation: A check for robustness\nTo test robustness to non‑stationarity, results are aggregated over 5 consecutive days per seed (20 seeds total), mimicking week‑level operations. Variance increases, but rank ordering and qualitative behavior remain unchanged.\n\n\n\n\n\n\n\n\n\n\nPolicy\nNet Profit\nAbandoned\nAvg Wait (s)\nWaste\n\n\n\n\nNewsvendor weekly\n25.58\n0.4%\n11.5\n40.3%\n\n\nPPO weekly\n20.17\n79.8%\n50.6\n52.6%\n\n\nPPO+Price weekly\n22.68\n81.7%\n52.0\n38.6%\n\n\nPPO+Par weekly\n-20.74\n90.9%\n55.7\n82.8%\n\n\nPPO+Price (profit-only) weekly\n26.36\n83.4%\n52.8\n44.1%\n\n\nQRDQN weekly\n29.04\n21.6%\n22.9\n46.9%\n\n\n\nRankings persist across seeds and week windows, and paired 95% CIs exclude zero, indicating the gaps reflect the reward/capacity structure rather than random variation."
  },
  {
    "objectID": "results.html#statistical-analysis-per-seed",
    "href": "results.html#statistical-analysis-per-seed",
    "title": "Results",
    "section": "Statistical analysis (per-seed)",
    "text": "Statistical analysis (per-seed)\nPer-seed mean profit and 95% confidence intervals are computed (normal approximation over seed means), and paired differences versus Newsvendor are reported (bootstrap 95% CIs).\n\n\n\nPolicy\nSeeds (n)\nMean profit\n95% CI (±)\n\n\n\n\nNewsvendor\n10\n103.21\n3.95\n\n\nPPO\n10\n35.28\n1.07\n\n\nQRDQN\n10\n41.32\n11.76\n\n\n\nPaired differences vs Newsvendor (per-seed): - PPO − Newsvendor: −67.94 (95% CI [−72.11, −63.68], n=10) - QRDQN − Newsvendor: −61.89 (95% CI [−71.45, −51.60], n=10)\nThese intervals confirm the large gaps observed in the tables and are not explained by sampling noise."
  },
  {
    "objectID": "results.html#training-dynamics",
    "href": "results.html#training-dynamics",
    "title": "Results",
    "section": "Training Dynamics",
    "text": "Training Dynamics\nTraining stability is assessed via rollout and evaluation reward curves.\n\n\nAcross methods, there is no divergence or collapse; learning curves are smooth, and observed behavior follows from reward alignment rather than optimization failure."
  },
  {
    "objectID": "results.html#demonstrations-of-trained-policies",
    "href": "results.html#demonstrations-of-trained-policies",
    "title": "Results",
    "section": "Demonstrations of Trained Policies",
    "text": "Demonstrations of Trained Policies\n\nPPO Policy Demo\n\n\n\nQRDQN Policy Demo"
  },
  {
    "objectID": "results.html#findings",
    "href": "results.html#findings",
    "title": "Results",
    "section": "Findings",
    "text": "Findings\n\nReward alignment drives learned behavior: PPO follows the incentives embedded in the shaped reward. Because the reward penalizes waiting, abandonment, and leftover inventory, the agent converges to a conservative production policy. Under-production is not a failure mode; it is the logical outcome of optimizing a reward that treats waste as worse than missed sales.\nHeuristic baselines outperform deep RL under misaligned objectives: Newsvendor succeeds because it encodes domain knowledge that the RL agents are never given: in this system, throughput matters more than waste. Its structural prior—bake aggressively and accept inventory losses—matches the true profit objective, while PPO and QRDQN optimize a proxy objective with different priorities.\nPricing adjustments offer limited gains without objective alignment: Bandit-based pricing lifts profit only when updates use profit directly. Even then, pricing cannot fix PPO’s core behavioral bias toward minimal production. The bandit can shift margins but cannot change the underlying production strategy learned from the shaped reward.\nTraining stability is not the bottleneck: Learning curves are smooth across PPO, PPO+Price, and QRDQN. There is no evidence of instability, divergence, or collapse. The gap between heuristics and RL arises from what the agents are asked to optimize, not from failures in optimization.\n\nTaken together, these findings show that beating strong operational heuristics requires either a reward aligned with business KPIs, RL methods that encode queue and capacity structure explicitly, or hybrid approaches that integrate domain priors. Algorithmic sophistication alone cannot overcome a reward function that penalizes the very behaviors needed for profit."
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "Methods",
    "section": "",
    "text": "This section describes the learning algorithms, pricing bandit, heuristic baselines, and experimental protocols used to evaluate control policies in Bellman’s Bakery. Two reinforcement learning (RL) agents—Proximal Policy Optimization (PPO) and Quantile Regression DQN (QRDQN), are compared against bandit-augmented PPO and established heuristics. All approaches interact with the same simulator described in Environment."
  },
  {
    "objectID": "methods.html#environment-summary-abridged",
    "href": "methods.html#environment-summary-abridged",
    "title": "Methods",
    "section": "Environment summary (abridged)",
    "text": "Environment summary (abridged)\nEach episode spans 240 ticks (10 seconds each) and includes two ovens (capacity 4 units each), a queue capped at 12 customers, and a limit of three customers served per tick. Arrivals follow a nonhomogeneous Poisson process with morning and midday peaks. Demand is nonstationary, exhibiting ±10% daily drift and ±10% weekly fluctuations.\nThe reward is a shaped, multi-objective signal:\n- revenue and a +0.1 serve bonus,\n- a wait penalty of 0.01 per tick per queued customer,\n- a 0.5 abandonment penalty,\n- a 0.1 balk penalty, and\n- a terminal leftover-cost penalty.\nDaily price multipliers scale revenue. Demand elasticity is minimal, so pricing primarily adjusts revenue rather than arrival intensity."
  },
  {
    "objectID": "methods.html#learning-agents",
    "href": "methods.html#learning-agents",
    "title": "Methods",
    "section": "Learning agents",
    "text": "Learning agents\n\nProximal Policy Optimization (PPO)\nPPO is implemented with MaskablePPO (SB3-Contrib) to ensure feasibility in the 11-action discrete space. Invalid actions are masked by assigning logits of \\(-\\infty\\). In addition, when any serve action is available, all bake actions are masked to enforce service priority.\n\nArchitecture and training setup\n\nPolicy network: two-layer MLP (256–256), Tanh activations.\n\nValue network: shares backbone features with the policy.\n\nVectorized training: 8 parallel environments.\n\n\n\nHyperparameters\n\nn_steps = 2048\n\nbatch_size = 256\n\ngamma = 0.995\n\ngae_lambda = 0.95\n\nclip_range = 0.2\n\nent_coef = 0.01\n\nlearning_rate = 3e-4\n\nTraining horizon: \\(10^6\\) environment steps (preceded by a 300k warm-up run).\n\n\n\nEnvironment-specific guardrails\n\nBake actions masked when any serve action is feasible.\n\nOven-capacity masking for bake actions.\n\nPer-tick cap: serve_per_tick = 3.\n\nPPO maximizes the shaped reward; it is not directly trained to maximize economic profit.\n\n\n\nQuantile Regression DQN (QRDQN)\nQRDQN estimates full return distributions by learning quantile values. In SB3, QRDQN does not natively support action masking; when an invalid action is selected, the environment applies a small penalty and treats the step as idle. Exploration follows ε-greedy decay.\n\nArchitecture and training setup\n\nMLP architecture: 256–256, Tanh activation.\n\nNumber of quantiles: 51.\n\nReplay buffer: size \\(10^6\\), uniform sampling.\n\nExploration: ε-greedy (1.0 → 0.05 linear decay).\n\nTarget network: Polyak averaging with \\(\\tau = 0.005\\).\n\nOptimizer: Adam, learning rate \\(3\\times 10^{-4}\\).\n\n\n\nHyperparameters\n\nbatch_size = 256\n\ngamma = 0.995\n\nlearning_starts = 50_000\n\ntrain_freq = 4, gradient_steps = 1\n\ntarget_update_interval = 1\n\nTraining horizon: \\(10^6\\) steps (matched to PPO)."
  },
  {
    "objectID": "methods.html#bandit-layer-daily-pricing",
    "href": "methods.html#bandit-layer-daily-pricing",
    "title": "Methods",
    "section": "Bandit layer: daily pricing",
    "text": "Bandit layer: daily pricing\nA Thompson-sampling bandit selects a single daily price multiplier, yielding a two-time-scale design: the bandit chooses the global price, and the RL controller operates within the day.\n\nPrice arms\n\nStandard grid: {0.9, 1.0, 1.1}\n\nProfit-oriented grid: {0.7, 0.85, 1.0, 1.15, 1.3}\n\n\n\nUpdate metrics\nTwo evaluation signals are considered:\n\nComposite metric (aligned with RL reward shaping):\n\\[\n\\text{metric}\n  = \\text{profit}\n- 0.02 \\cdot \\text{wait\\_ticks}\n- 5 \\cdot \\text{abandoned}\n- 2 \\cdot \\text{balked}.   \n  \\]\nNet-profit-only metric:\n\\[\n\\text{metric} = \\text{net profit}.\n\\]\n\nThe profit-only metric tests whether the bandit can push PPO toward more revenue-oriented behavior."
  },
  {
    "objectID": "methods.html#heuristic-baselines",
    "href": "methods.html#heuristic-baselines",
    "title": "Methods",
    "section": "Heuristic baselines",
    "text": "Heuristic baselines\n\nBake-to-Par\nMaintains fixed per-item inventory targets. The agent bakes the largest deficit first and serves the earliest matching customer.\n\n\nGreedy Queue\nCounts pending requests in the queue and bakes the most requested item; defaults to long-run demand proportions when no item is dominant.\n\n\nNewsvendor (strongest baseline)\nComputes per-item production targets from expected total demand and bakes toward the largest remaining target gap. This heuristic performs strongly in settings where demand is predictable and waste costs are absorbed by high throughput."
  },
  {
    "objectID": "methods.html#training-protocol",
    "href": "methods.html#training-protocol",
    "title": "Methods",
    "section": "Training protocol",
    "text": "Training protocol\nAll learning agents use identical simulator configurations. Training employs 8 vectorized environments and Adam optimization. Models are checkpointed periodically. Reported results include:\n\nthe final checkpoint, and\n\nthe best checkpoint, selected using evaluation net profit averaged over seeds.\n\nRandom seed handling follows SB3 conventions for PPO and QRDQN to ensure reproducibility."
  },
  {
    "objectID": "methods.html#evaluation-protocol",
    "href": "methods.html#evaluation-protocol",
    "title": "Methods",
    "section": "Evaluation protocol",
    "text": "Evaluation protocol\nTwo evaluation regimes are used:\n\nFull horizon: 10 seeds × 20 days; results reported as seed-level means.\n\nWeekly stability: 20 seeds × 5 days; emphasizes variance across seeds.\n\nKey performance indicators (KPIs):\n\nnet profit,\n\nabandonment rate,\n\nmean wait (seconds),\n\nwaste rate,\n\nservice rate.\n\nRL-internal performance is monitored through TensorBoard rollout and evaluation reward curves."
  },
  {
    "objectID": "methods.html#reward-specification-reference",
    "href": "methods.html#reward-specification-reference",
    "title": "Methods",
    "section": "Reward specification (reference)",
    "text": "Reward specification (reference)\n\n\n\nComponent\nMeaning\nWeight\nSign\n\n\n\n\nProfit\nrevenue − cost\n+1.0\npositive\n\n\nWait penalty\nper tick of waiting\n−0.01\nnegative\n\n\nAbandon penalty\nexpired patience\n−0.7\nnegative\n\n\nBalk penalty\nqueue full\n−0.1\nnegative\n\n\nLeftover cost\nunsold items\n−1.0\nnegative\n\n\nServe bonus\neach service\n+0.1\npositive\n\n\n\nFormal definition: \\[\nR_t\n= \\mathrm{profit}_t\n- \\lambda_w\\,\\mathrm{wait\\_ticks}_t\n- \\lambda_a\\,\\mathrm{abandon}_t\n- \\lambda_b\\,\\mathrm{balk}_t\n- \\lambda_L\\,\\mathrm{leftover}_t\n+ \\lambda_s\\,\\mathrm{served}_t,\n\\]\nwith\n\\(\\lambda_w = 0.01\\),\n\\(\\lambda_a = 0.7\\), want e \\(\\lambda_b = 0.1\\),\n\\(\\lambda_L = 1.0\\),\n\\(\\lambda_s = 0.1\\).\n\nExpected behavioral differences\n\nPPO tends to adopt conservative, waste-minimizing strategies because penalties accumulate over time.\n\nQRDQN, with replay and distributional targets, may explore more aggressive bake/serve trade-offs even under the same reward."
  },
  {
    "objectID": "methods.html#implementation-details-and-reproducibility",
    "href": "methods.html#implementation-details-and-reproducibility",
    "title": "Methods",
    "section": "Implementation details and reproducibility",
    "text": "Implementation details and reproducibility\n\nFrameworks: Stable-Baselines3 2.3.0 and SB3-Contrib 2.3.0.\n\nCore stack pinned: NumPy 1.26.4, Pandas 2.1.4, PyArrow 14.0.2.\n\nTraining and evaluation scripts, seed configurations, and plotting utilities are included in project repositories.\n\nTensorBoard logs are exported for inclusion in the Results section."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reward Misalignment in Deep Reinforcement Learning: Evidence from a Bakery Production Environment",
    "section": "",
    "text": "This project examines reward–behavior alignment in a capacity‑constrained production setting. A custom “Bellman’s Bakery” environment models a single day with two ovens, stochastic arrivals, patience/abandonment, and five products. PPO and QRDQN are evaluated against heuristics. Under a shaped reward that heavily penalizes leftovers and delay, learning converges to conservative policies that under‑produce to avoid waste. Pricing adjustments provide limited leverage because oven throughput is the binding constraint. Profit‑aligned evaluation yields modest increases in service and profit but does not remove the bottleneck. Findings highlight that reward design and structural constraints dominate performance more than the specific deep RL algorithm."
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "Reward Misalignment in Deep Reinforcement Learning: Evidence from a Bakery Production Environment",
    "section": "",
    "text": "This project examines reward–behavior alignment in a capacity‑constrained production setting. A custom “Bellman’s Bakery” environment models a single day with two ovens, stochastic arrivals, patience/abandonment, and five products. PPO and QRDQN are evaluated against heuristics. Under a shaped reward that heavily penalizes leftovers and delay, learning converges to conservative policies that under‑produce to avoid waste. Pricing adjustments provide limited leverage because oven throughput is the binding constraint. Profit‑aligned evaluation yields modest increases in service and profit but does not remove the bottleneck. Findings highlight that reward design and structural constraints dominate performance more than the specific deep RL algorithm."
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Conclusion and Future Work",
    "section": "",
    "text": "The results highlight a central theme running through reinforcement learning for operations problems: an agent’s behavior reflects the reward it is given, not the metric by which humans ultimately judge success. In the Tiny Bakery environment, the Newsvendor heuristic achieves the highest profit precisely because it embraces a strategy that the RL reward discourages—producing aggressively, holding inventory buffers, and accepting waste as a necessary cost of maintaining service levels.\nPPO, by contrast, optimizes a composite reward that places substantial weight on wait times, abandonment, and leftover inventory. Within that objective, the agent behaves exactly as expected: it adopts a conservative baking policy that minimizes exposure to negative reward terms but sacrifices throughput, service quality, and ultimately profit. The pricing bandit reinforces this pattern when trained on the same composite reward, and even the profit-only version can only shift behavior modestly because pricing cannot relieve the underlying capacity bottlenecks in ovens and queue dynamics. These findings echo trends reported in recent work on RL for perishable goods, service systems, and dynamic pricing, where agents often appear “suboptimal” on business metrics while being entirely optimal under the shaped reward.\nRather than identifying algorithmic failure, the experiments surface the deeper issue of objective misalignment—a challenge that is widely recognized in the RL-for-operations literature and remains an active research area."
  },
  {
    "objectID": "conclusion.html#future-directions",
    "href": "conclusion.html#future-directions",
    "title": "Conclusion and Future Work",
    "section": "Future directions",
    "text": "Future directions\nSeveral extensions would help address this alignment gap and explore richer learning dynamics in the environment.\n1. Reward redesign and constrained objectives.\nA profit-aligned reward or a constrained RL formulation—e.g., profit maximization subject to service-level penalties—could shift the agent toward more economically meaningful policies. Lagrangian relaxation, CVaR-based objectives, or multi-objective reward shaping may offer more principled ways to encode trade-offs between waste, service quality, and inventory risk.\n2. Curriculum training and environmental structure.\nA staged curriculum beginning with stationary demand and gradually introducing non-stationarity may guide the policy out of the conservative local optimum. This mirrors approaches used successfully in perishable-goods RL and may reduce the tendency to collapse into underproduction.\n3. Capacity-aware strategies.\nIntroducing mechanisms such as short-horizon lookahead, bake-batch optimization, or alternative oven configurations may give the agent tools to escape starvation equilibria. Allowing limited same-day restocking or flexible par adjustments could further expand the feasible policy space.\n4. Improved baselines and hybrid methods.\nMore expressive par-selection heuristics, contextual bandits for dynamic pricing, or hybrid RL–heuristic controllers could bridge the gap between flexible learning and domain expertise. Because Newsvendor performs well by exploiting strong structural priors, combining such priors with RL may yield more robust strategies.\n5. Alternative RL algorithms (QRDQN, SAC variants).\nValue-based or distributional methods may behave differently under the same reward structure, particularly when long-tail return distributions matter. Once QRDQN experiments complete, comparing its learned policies with PPO will help determine whether the conservative bias is algorithm-specific or fundamentally tied to the reward.\nTaken together, these findings suggest that the primary challenge in this domain is not improving PPO or adding more architectural sophistication, but determining how to encode the bakery’s operational objectives into a reward formulation the agent can meaningfully optimize. The experiments here provide a detailed diagnosis of the misalignment and lay the groundwork for the next generation of RL agents that better reflect real-world business priorities."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis & Discussion",
    "section": "",
    "text": "The empirical findings from Bellman’s Bakery demonstrate a distinct and consistent pattern: the reinforcement learning (RL) agents perform exactly as trained, optimizing their shaped reward functions with precision. That being said, this behavior is often not what would be considered economically rational when looking at business metrics like net profit. The following is a summary of what was observed in PPO behavior, how pricing affects it, and how it relates to other work."
  },
  {
    "objectID": "analysis.html#behavior-of-ppo-in-a-capacity-constrained-bakery",
    "href": "analysis.html#behavior-of-ppo-in-a-capacity-constrained-bakery",
    "title": "Analysis & Discussion",
    "section": "Behavior of PPO in a Capacity-Constrained Bakery",
    "text": "Behavior of PPO in a Capacity-Constrained Bakery\nAcross all PPO variants, the agent consistently converges to a policy that minimizes penalties for waste, abandonment, and wait times by sharply limiting production. This outcome should not be misread as a failure to learn, rather, it is a rational response to the shaped reward, which imposes strong penalties on leftover/wasted inventory and only modest gains for additional service or throughput.\nWhen oven space is limited and stoachastic customer arrivals are a factor, making excess production leads to wasted stock. On the flip side, producing less cuts down on losses, stabilizing rewards despite shifting demand. This pushes agents toward keeping output low, cutting both inventory and waste, even if it means more lost sales or longer queues.\nThis behavior effectively represents a risk-averse approximation of the shaped reward, reflecting how agents prioritize safety and penalty avoidance over reward maximization (Garcı́a and Fernández 2015). PPO is not learning a profit-maximizing policy; it is optimizing a composite signal that encodes waste aversion more strongly than revenue generation. This aligns with safe RL formulations where reward signals bias agents toward conservative behavior under uncertainty (Garcı́a and Fernández 2015).\nIn earlier versions of the environment, a small idle penalty was included to discourage the agent from stalling. Ironically, this had the opposite effect: it pushed the agent to avoid acting altogether when uncertain, over-prioritizing idling to sidestep the risk of incurring other penalties. After removing this penalty and implementing serve-first action masking to encourage throughput, idling was substantially reduced—but the broader production strategy stayed the same. This makes clear that the underlying issue was never policy instability or poor convergence but that the agent was behaving rationally under the incentives it was given.\nThis shows a bigger structural problem: when the reward function encodes a distorted set of trade-offs, agents learn policies that don’t work. In this instance, the RL agent learned to avoid waste so strongly that it stopped production, even when there was a lot of demand. This illustrates a well-documented issue in the reward shaping literature: if the shaped reward fails to maintain the optimal policy under transformation, even minor misalignments can result in learned behaviors that significantly deviate from human designer expectations (Ng, Harada, and Russell 1999)."
  },
  {
    "objectID": "analysis.html#misalignment-between-reward-and-business-metrics",
    "href": "analysis.html#misalignment-between-reward-and-business-metrics",
    "title": "Analysis & Discussion",
    "section": "Misalignment Between Reward and Business Metrics",
    "text": "Misalignment Between Reward and Business Metrics\nWhile the true business goal is to maximize net profit while preserving service quality, the shaped reward used during training captures a bundle of operational priorities:\n\nper-tick wait cost\n\nabandonment penalty\n\nbalking penalty\n\nleftover inventory cost\n\na modest serve bonus\n\nThe purpose of this composite objective was to make learning more stable and make operational realism more real. But it unintentionally overweights penalties for waste and delay, which makes the agent think that being cautious is the best way to act. In a bakery like this one, where there is a lot of variation and not much room for error, the best way to run a business is often to “bake ahead of demand” and accept some waste in exchange for throughput. The reward stops this from happening, which leads to learned policies that seem to be bad for profits, even though they are the best for the shaped goal."
  },
  {
    "objectID": "analysis.html#limited-effectiveness-of-pricing-adaptation",
    "href": "analysis.html#limited-effectiveness-of-pricing-adaptation",
    "title": "Analysis & Discussion",
    "section": "Limited Effectiveness of Pricing Adaptation",
    "text": "Limited Effectiveness of Pricing Adaptation\nTwo bandit-based pricing variants were evaluated to test whether dynamic price adjustments could steer behavior toward greater profitability. Results show that pricing only improves performance when price changes shift demand volume in a meaningful way. In Bellman’s Bakery, the binding constraint is service capacity—not demand.\nWhen the pricing bandit is trained using the same shaped reward, it reinforces PPO’s conservative tendencies. Price adjustments do not relieve bottlenecks in baking or queuing; instead, they subtly reinforce waste aversion. Switching the bandit to optimize directly for profit produces the expected response: slightly higher margins and slightly more waste, indicating that the agent now tolerates higher risk. However, the net impact remains small, as pricing cannot resolve a bottleneck caused by physical capacity.\nThis is consistent with dynamic pricing literature in constrained systems: pricing only works when it can influence the actual point of friction, which in this case is not price elasticity but limited service throughput."
  },
  {
    "objectID": "analysis.html#ablation-par-level-hybrid-behavior",
    "href": "analysis.html#ablation-par-level-hybrid-behavior",
    "title": "Analysis & Discussion",
    "section": "Ablation: Par-Level Hybrid Behavior",
    "text": "Ablation: Par-Level Hybrid Behavior\nThe PPO+Par hybrid agent significantly underperforms relative to both standalone PPO and heuristic baselines. Par-level logic was introduced as an interpretable heuristic, but it imposes a rigid production ceiling that fails to adapt to demand fluctuations. The par mechanism enforces persistent underproduction, locking the agent into a starvation regime from which neither the policy nor the heuristic can recover.\nWhile additional tuning might improve this baseline, the deeper issue lies in its inflexibility. In non-stationary environments, policies must adapt dynamically to demand drift. Par logic, as implemented here, lacks that adaptability."
  },
  {
    "objectID": "analysis.html#placeholder-qrdqn-analysis-to-be-completed",
    "href": "analysis.html#placeholder-qrdqn-analysis-to-be-completed",
    "title": "Analysis and Discussion",
    "section": "",
    "text": "A section will be added here once QRDQN training concludes. The analysis will compare value-based and policy-gradient behavior in this environment and assess whether distributional value estimation mitigates conservative tendencies seen in PPO."
  },
  {
    "objectID": "analysis.html#connection-to-related-literature",
    "href": "analysis.html#connection-to-related-literature",
    "title": "Analysis & Discussion",
    "section": "Connection to Related Literature",
    "text": "Connection to Related Literature\nThese empirical patterns reflect and reinforce themes in the broader literature on reinforcement learning in operations, inventory management, and pricing.\nNomura et al. (2025) show that PPO can approximate dynamic programming baselines in perishable inventory problems—but only when the reward function is carefully aligned with profit objectives. When penalties are imbalanced, PPO converges to behavior that looks inefficient through a business lens but is, in fact, optimal under the reward. They also find that pricing changes only influence outcomes when demand elasticity is meaningful, which is consistent with these results.\nvan Hezewijk (2024) examines RL in service systems and finds that agents frequently over-prioritize penalty avoidance when shaping rewards are used. Specifically, leftover costs and delay penalties tend to dominate learning, leading agents to suppress production or service rates—even when that reduces net throughput. This mirrors the behavior observed here.\nGenalti (2021) emphasizes that bandit-based dynamic pricing is only effective when it acts on the root constraint. In cases where congestion or physical capacity limits the system, pricing alone cannot move the needle. This is precisely what is observed: profit-based bandits respond rationally, but cannot overcome a throughput ceiling set by bake times and queueing limits."
  },
  {
    "objectID": "analysis.html#overall-interpretation",
    "href": "analysis.html#overall-interpretation",
    "title": "Analysis & Discussion",
    "section": "Overall Interpretation",
    "text": "Overall Interpretation\nTaken together, these findings tell a coherent story. The RL agents are not failing to learn—they are learning exactly what they are asked to optimize. The issue lies in how the reward is constructed, not in the optimization algorithm itself. PPO performs well against its shaped reward, but that reward underrepresents the value of throughput and overrepresents the cost of waste and delay. As a result, the agent learns to avoid waste at all costs, even when doing so reduces profit.\nPricing cannot solve the problem because the true bottleneck is structural, not demand-driven. Hybrid logic like par-level control underperforms because it is too rigid to respond to uncertainty. Meanwhile, heuristic strategies like Newsvendor outperform RL agents because they implicitly encode the right structural prior: in a queueing bakery with non-stationary demand, some overproduction is not wasteful—it is necessary.\nConclusion: The lesson here is not that deep RL “doesn’t work,” but that reward design is the true optimization problem. Bridging the gap between algorithmic performance and real-world business value requires either (1) better-aligned reward functions, (2) constrained or risk-sensitive RL methods, or (3) hybrid strategies that incorporate domain knowledge. Without these, even the best algorithms will learn the wrong lesson."
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "PPO: n_steps=2048, batch_size=256, gamma=0.995, gae_lambda=0.95, ent_coef=0.01.\nBandit arms: {0.9,1.0,1.1} (penalty metric), {0.7,0.85,1.0,1.15,1.3} (profit metric)."
  },
  {
    "objectID": "appendix.html#a.-hyperparameters",
    "href": "appendix.html#a.-hyperparameters",
    "title": "Appendix",
    "section": "",
    "text": "PPO: n_steps=2048, batch_size=256, gamma=0.995, gae_lambda=0.95, ent_coef=0.01.\nBandit arms: {0.9,1.0,1.1} (penalty metric), {0.7,0.85,1.0,1.15,1.3} (profit metric)."
  },
  {
    "objectID": "appendix.html#b.-environment-configurations",
    "href": "appendix.html#b.-environment-configurations",
    "title": "Appendix",
    "section": "B. Environment Configurations",
    "text": "B. Environment Configurations\n\nserve_per_tick=3; 2 ovens, capacity 4u each; patience 30–90s; queue cap 12; non‑stationarity on.\nPrices, costs, yields, sizes as specified in code constants."
  },
  {
    "objectID": "appendix.html#c.-additional-figures",
    "href": "appendix.html#c.-additional-figures",
    "title": "Appendix",
    "section": "C. Additional Figures",
    "text": "C. Additional Figures\n\nSee reports/figs/* for exported TensorBoard curves per run."
  },
  {
    "objectID": "environment.html",
    "href": "environment.html",
    "title": "Environment",
    "section": "",
    "text": "The Bellman’s Bakery environment models a capacity-constrained, perishable-goods service system with stochastic arrivals, queue dynamics, limited oven capacity, and nonstationary demand shifts. Each episode simulates a single operating day of 240 decision ticks (10 seconds each). At each tick the agent selects a discrete action—serving, baking, or idling—subject to inventory, queue state, and oven capacity.\nThe environment exposes operational trade-offs among production timing, waste, congestion, abandonment, and responsiveness to shifting demand."
  },
  {
    "objectID": "environment.html#viewer-snapshot",
    "href": "environment.html#viewer-snapshot",
    "title": "Environment",
    "section": "Viewer Snapshot",
    "text": "Viewer Snapshot"
  },
  {
    "objectID": "environment.html#state-actions-and-dynamics",
    "href": "environment.html#state-actions-and-dynamics",
    "title": "Environment",
    "section": "State, Actions, and Dynamics",
    "text": "State, Actions, and Dynamics\n\nState Representation\nLet \\(s_t\\) denote the state at tick \\(t\\).\nThe observation vector includes:\n\nInventory counts for all 5 items\n\nOven states: remaining-time fractions for 2 ovens\n\nQueue features:\n\nqueue length (cap 12)\n\ntop \\(K = 5\\) customers, each with\n\ndesired-item one-hot (5)\n\nremaining-patience fraction\n\n\n\nTime-of-day: sine and cosine encodings\n\nDaily price multiplier\n\nNonstationary context: ±10% daily drift, ±10% weekly item swings\n\n\n\nAction Space\nThere are 11 discrete actions, with masking of infeasible choices:\n1–5. Serve item \\(i\\)\n6–10. Bake item \\(i\\)\n11. Idle\nUp to 3 customers may be served per tick (serve_per_tick = 3).\nMasking applies \\(-\\infty\\) to logits of invalid actions.\nWhen any serve action is valid, bake actions are masked out to prioritize service.\n\n\nTransition Dynamics\n\nHorizon: 240 ticks\n\nArrivals: Poisson with morning and lunch peaks\n\nQueue cap: 12; overflow customers balk\n\nPatience: 30–90 s; expired customers abandon\n\nProduction: item-specific sizes, bake times, and yields\n\nEnd-of-day waste: leftover items incur cost\n\nRestocks: disabled\n\nNonstationarity: ±10% daily drift, ±10% weekly item shifts"
  },
  {
    "objectID": "environment.html#menu-capacities-and-parameters",
    "href": "environment.html#menu-capacities-and-parameters",
    "title": "Environment",
    "section": "Menu, Capacities, and Parameters",
    "text": "Menu, Capacities, and Parameters\n\nMenu Items\n\n\n\n\n\n\n\n\n\n\n\nItem\nSize (u)\nBake Time (s)\nBatch Yield\nBase Price\nCost (40%)\n\n\n\n\nMini Red Velvet Cake\n3\n90\n2\n$6.00\n$2.40\n\n\nRaspberry Matcha Roll\n1.5\n60\n4\n$4.50\n$1.80\n\n\nStrawberry Cream Slice\n1\n36\n4\n$3.50\n$1.40\n\n\nChocolate & Almond Drip Cake\n4\n120\n1\n$8.00\n$3.20\n\n\nChocolate Orange Roll\n1.5\n60\n4\n$4.50\n$1.80\n\n\n\n\n\nOvens & Initial Conditions\n\nOvens: 2\n\nCapacity: 4 units each\n\nStarting inventory: Strawberry=6, Matcha=4, Orange=4, Red Velvet=2, Drip Cake=1\n\n\n\nDemand & Pricing\n\nDemand mix: 30% Strawberry, 20% Matcha, 20% Orange, 15% Red Velvet, 15% Drip Cake\n\nAverage customers/day: ~60 (with morning and lunch peaks)\n\nDaily price multipliers: {0.9, 1.0, 1.1}\n\nPrice elasticity: none in this version — multipliers scale revenue only (demand unchanged)"
  },
  {
    "objectID": "environment.html#reward-function",
    "href": "environment.html#reward-function",
    "title": "Environment",
    "section": "Reward Function",
    "text": "Reward Function\nThe per-tick reward \\(r_t\\) is:\n\\[\nr_t = (\\text{price}_i - \\text{cost}_i)\\mathbf{1}\\{\\text{serve } i\\}\n+ 0.1 \\mathbf{1}\\{\\text{serve } i\\}\n- 0.01 \\cdot \\text{queue\\_length}_t\n- 0.7 \\mathbf{1}\\{\\text{abandon}\\}\n- 0.1 \\mathbf{1}\\{\\text{balk}\\}.\n\\]\nAt episode end:\n\\[\nr_{\\text{end}} = -\\sum_i \\text{cost}_i \\cdot \\text{leftover}_i.\n\\]\nNo idle penalty is used.\n\nInterpretation\n\nRevenue and service events contribute positively.\n\nQueueing, abandonment, and balking incur penalties.\n\nEnd-of-day leftovers add cost.\n\nIllegal actions are softly penalized and treated as idle.\n\nBecause leftover cost equals full ingredient cost, the reward overweights waste avoidance, biasing the agent toward conservative baking."
  },
  {
    "objectID": "environment.html#business-objective-vs-rl-objective",
    "href": "environment.html#business-objective-vs-rl-objective",
    "title": "Environment",
    "section": "Business Objective vs RL Objective",
    "text": "Business Objective vs RL Objective\n\nBusiness KPI: maximize net profit with acceptable wait and abandonment levels.\n\nRL objective: maximize the shaped reward above.\n\nThe shaped reward trades off multiple goals and is not identical to maximizing net profit. This misalignment explains PPO’s conservative behavior despite appearing “irrational” from a profit perspective."
  },
  {
    "objectID": "environment.html#observationaction-interface",
    "href": "environment.html#observationaction-interface",
    "title": "Environment",
    "section": "Observation–Action Interface",
    "text": "Observation–Action Interface\nThe environment supplies:\n\nstructured vector observations\n\nan 11-action discrete space with masking\n\nfull MDP transitions\n\nSuitable for PPO, QRDQN, and entropy-regularized methods."
  },
  {
    "objectID": "environment.html#training-configuration",
    "href": "environment.html#training-configuration",
    "title": "Environment",
    "section": "Training Configuration",
    "text": "Training Configuration\n\nAlgorithm: PPO (Stable-Baselines3) with masking\n\nParallel envs: 8\n\nTraining budget: \\(3\\times10^5\\) → \\(10^6\\) steps\n\nEvaluation metrics: net profit, wait time, abandonment, waste rate, service level\n\nPPO provides stable learning in long-horizon, stochastic, discrete-action settings but cannot compensate for reward–metric misalignment."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Operational choices in retail or service settings typically involve balancing multiple competing goals, such as increasing revenue, reducing customer wait times, preventing stockouts, and minimizing spoilage. Bakeries, for example, face short-term challenges with shelf-life items, limited space, and unpredictable demand that varies by hour. The well-known newsvendor model provides clear guidance for single-period ordering decisions, offering analytically optimal stocking levels under uncertain demand (Khouja 1999) (Arrow, Harris, and Marschak 1951). However, these formulations focus primarily on a single objective and do not account for real-time dynamics such as delays, customer abandonment, congestion, or operational bottlenecks.\nDeep reinforcement learning (DRL) has emerged as a useful method for developing control strategies through direct experience in complex settings. While these approaches can capture responses to shifting demand patterns, delays, or system limits, even without predefined models, they do not inherently aim to achieve goals like revenue or performance. DRL agents are only as good as the reward function they are trained on. They do not “optimize for profit” or “maximize customer satisfaction” unless those goals are explicitly and precisely defined. If the reward signal captures only part of what matters to a business, the resulting behavior can end up looking nothing like what a human decision-maker would actually want."
  },
  {
    "objectID": "introduction.html#motivation",
    "href": "introduction.html#motivation",
    "title": "Introduction",
    "section": "",
    "text": "Operational choices in retail or service settings typically involve balancing multiple competing goals, such as increasing revenue, reducing customer wait times, preventing stockouts, and minimizing spoilage. Bakeries, for example, face short-term challenges with shelf-life items, limited space, and unpredictable demand that varies by hour. The well-known newsvendor model provides clear guidance for single-period ordering decisions, offering analytically optimal stocking levels under uncertain demand (Khouja 1999) (Arrow, Harris, and Marschak 1951). However, these formulations focus primarily on a single objective and do not account for real-time dynamics such as delays, customer abandonment, congestion, or operational bottlenecks.\nDeep reinforcement learning (DRL) has emerged as a useful method for developing control strategies through direct experience in complex settings. While these approaches can capture responses to shifting demand patterns, delays, or system limits, even without predefined models, they do not inherently aim to achieve goals like revenue or performance. DRL agents are only as good as the reward function they are trained on. They do not “optimize for profit” or “maximize customer satisfaction” unless those goals are explicitly and precisely defined. If the reward signal captures only part of what matters to a business, the resulting behavior can end up looking nothing like what a human decision-maker would actually want."
  },
  {
    "objectID": "introduction.html#problem-formulation",
    "href": "introduction.html#problem-formulation",
    "title": "Introduction",
    "section": "Problem Formulation",
    "text": "Problem Formulation\nBellman’s Bakery is a custom OpenAI Gymnasium environment designed for this study to simulate a small bakery with capacity constraints, where each episode corresponds to one single day of operation. Customers arrive with stochastic demand and limited patience; they may leave at the sight of a long queue or if their wait times exceed a certain threshold. The agent(baker) decides which items to produce, when to serve customers, and when to idle, all while operating under constraints such as oven capacity and fixed bake times. At the end of each day, unsold inventory is discarded. The environment records business KPIs such as net profit, average wait, abandonment, and waste, and uses an action mask to enforce valid decisions. Its reward is a combined (weighted) signal: revenue and successful service add points, while waiting, abandonment, balking, and leftover inventory subtract points. This reward is intentionally not equivalent to net profit. The central question is how different solution approaches, newsvendor-styleheuristics, Proximal Policy Optimization (PPO), PPO+bandits, and Quantile Regression Deep Q‑Network (QRDQN), optimize this shaped reward and how the resulting policies perform when judged against business-facing metrics such as profitability, service quality, and inventory efficiency.\nTo investigate this, four types of policies are benchmarked:\n\nA newsvendor-style heuristic that selects bake quantities using a static demand model and a target service level (quantile rule).\nA PPO agent trained directly on the shaped reward in the Bellman’s Bakery environment.\nA hybrid PPO+bandit variant that augments PPO with a pricing or par-level bandit component.\nA QRDQN agent, representing an off-policy, value-based DRL method adapted to the same discrete action space.\n\nThis setup leads to the following research questions:\n\nHow does reward shaping in a multi-objective production system influence the qualitative behavior of deep RL agents?\nUnder what conditions does a DRL policy outperform, match, or underperform a simple newsvendor heuristic when evaluated on profit, waiting time, abandonment, and waste?\nDo off-policy distributional methods such as QRDQN exhibit different trade-offs than on-policy methods like PPO under the same reward and environment?"
  },
  {
    "objectID": "introduction.html#related-work",
    "href": "introduction.html#related-work",
    "title": "Introduction",
    "section": "Related Work",
    "text": "Related Work\nDespite substantial work at the intersection of perishable inventory, dynamic pricing, and reinforcement learning, most studies analyze settings that differ from the operational context examined in this paper. Research on perishable inventory typically focuses on optimal ordering and pricing strategies under stochastic demand, often assuming well-structured demand models and low-dimensional state spaces.For instance, Nomura and colleagues examined a perishable inventory issue involving dynamic pricing and demand that varies by age comparing traditional dynamic programming approaches to PPO methods (Nomura, Liu, and Nishi 2025). In the end, they found that DRL could get close to the best possible results while simulataneously cutting down on the time needed for computations. Two conclusions are especially relevant here: (i) PPO performs well when the reward is tightly aligned with the economic objective, and (ii) reward design strongly shapes learned behavior.\nOther lines of research examine RL in queueing and scheduling environments. Van Hezewijk shows that RL agents often prioritize surrogate reward terms, such as penalties for waiting or abandonment, over business-facing performance metrics when these are not tightly aligned (Hezewijk} 2024). This raises concerns about whether RL agents may develop overly conservative or low-throughput policies in systems with capacity constraints. In parallel, dynamic pricing research has often employed bandit-based approaches rather than full RL. Genalti demonstrates that Thompson sampling can effectively learn profit-maximizing pricing strategies under uncertainty, especially when demand elasticity is monotonic and data are limited (Genalti 2021). The pricing bandit used in this study reflects that lineage, though prior work suggests its effectiveness depends on whether pricing can meaningfully shift demand.\nTaken together, these works indicate that RL methods are highly sensitive to reward specification and state–action structure, and that bandit-based pricing only adds value when price elasticity meaningfully shifts demand. Accordingly, this paper investigates three core questions in the context of a simulated, capacity-constrained bakery: (1) how PPO responds to a shaped reward that only partially reflects profit; (2) whether surrogate penalties for waiting and abandonment bias policies toward conservative, low-waste behavior; and (3) whether pricing flexibility improves performance when demand is uncertain but throughput is constrained by service capacity rather than elasticity."
  },
  {
    "objectID": "introduction.html#contributions-of-this-project",
    "href": "introduction.html#contributions-of-this-project",
    "title": "Introduction",
    "section": "Contributions of this project",
    "text": "Contributions of this project\nThis project brings forward three key contributions. One involves a detailed queue-based simulation for a small bakery operation capturing those real tensions in daily work, like balancing profit against service quality and cutting down on waste. Another contribution comes from a structured comparison of various policy approaches through experiments that cover basic heuristics, on-policy reinforcement learning with PPO, hybrid bandit, RL methods, and an off-policy distributional technique called QRDQN. All of them faced evaluation under identical reward shaping. The third point reveals what happens when training rewards drift away from the actual evaluation measures. PPO starts to chase the wrong targets too hard. It pushes to minimize waste, even if that means sacrificing profit and service levels. Meanwhile, QRDQN along with heuristic policies land at varied spots on the same trade-off line. Overall, these outcomes point out something crucial: reward design and matching objectives really matter in deep reinforcement learning for production and inventory setups."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References\n\nArrow, Kenneth J, Theodore Harris, and Jacob Marschak. 1951. “Optimal Inventory Policy.” Econometrica 19 (3): 250–72. https://www.or.mist.i.u-tokyo.ac.jp/takeda/FreshmanCourse/ArrowHarrisMarschak.pdf.\n\n\nGenalti, Gianmarco. 2021. “A Multi-Armed Bandit Approach to Dynamic Pricing.” Master's thesis, Laurea Magistrale in Mathematical Engineering - Ingegneria Matematica. https://www.politesi.polimi.it/bitstream/10589/183733/4/genalti_executive_summary.pdf.\n\n\nHezewijk}, Lotte {van. 2024. “Deep Reinforcement Learning for Production and Inventory Management: Bridging the Gap Between Theory and Practice.” Beta PhD Dissertation Series. Phd Thesis 1 (Research TU/e / Graduation TU/e), Industrial Engineering; Innovation Sciences; Eindhoven University of Technology.\n\n\nKhouja, Mohamed. 1999. “The Single-Period (Newsvendor) Problem: Literature Review and Suggestions for Future Research.” Omega 27 (5): 537–53. https://www-sciencedirect-com.proxy.library.georgetown.edu/science/article/pii/S0305048399000171.\n\n\nNomura, Yusuke, Ziang Liu, and Tatsushi Nishi. 2025. “Deep Reinforcement Learning for Dynamic Pricing and Ordering Policies in Perishable Inventory Management.” Applied Sciences 15 (5). https://doi.org/10.3390/app15052421."
  },
  {
    "objectID": "analysis.html#placeholder-qrdqn-analysis-pending-completion",
    "href": "analysis.html#placeholder-qrdqn-analysis-pending-completion",
    "title": "Analysis and Discussion",
    "section": "Placeholder: QRDQN Analysis (Pending Completion)",
    "text": "Placeholder: QRDQN Analysis (Pending Completion)\nA full analysis of QRDQN performance will be included once final training concludes. This section will compare value-based and policy-gradient approaches in the Bellman’s Bakery environment, with special attention to whether distributional value estimation helps mitigate the overly conservative tendencies observed in PPO."
  },
  {
    "objectID": "analysis.html#qrdqn-analysis",
    "href": "analysis.html#qrdqn-analysis",
    "title": "Analysis & Discussion",
    "section": "QRDQN Analysis",
    "text": "QRDQN Analysis\nQuantile Regression DQN (QRDQN) was trained under the same environment and reward shaping as PPO. The distributional critic reduced value‑estimate variance and produced policies that were marginally more willing to bake ahead of demand. The qualitative pattern nevertheless remained: when trained on the shaped reward, QRDQN also converged to a conservative regime that limits production to avoid leftover penalties.\nWhen QRDQN was paired with a profit‑aligned evaluation signal, modest gains were observed relative to the shaped‑reward variant (slightly higher service rates and profit, with slightly more waste). Improvements were incremental rather than transformative, indicating that distributional value estimation alone does not overcome the capacity constraint or reward misalignment. The results support the broader conclusion that reward design and structural bottlenecks—not the choice between policy‑gradient and value‑based methods—dominate performance in this domain."
  },
  {
    "objectID": "index.html#demo-optional",
    "href": "index.html#demo-optional",
    "title": "Reward Misalignment in Deep Reinforcement Learning: Evidence from a Bakery Production Environment",
    "section": "Demo (optional)",
    "text": "Demo (optional)\nIf a short viewer clip is available, it can be embedded below. This page expects the file at videos/viewer_index_demo.mp4 in the repo root.\n\nYour browser does not support the video tag.\n\nQuickstart and viewer controls are documented in the project README."
  },
  {
    "objectID": "index.html#table-of-contents",
    "href": "index.html#table-of-contents",
    "title": "Reward Misalignment in Deep Reinforcement Learning: Evidence from a Bakery Production Environment",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nIntroduction\nEnvironment\nMethods\nResults\nAnalysis & Discussion\nConclusion & Future Work\nReferences"
  },
  {
    "objectID": "index.html#bellmans-bakery-overview",
    "href": "index.html#bellmans-bakery-overview",
    "title": "Reward Misalignment in Deep Reinforcement Learning: Evidence from a Bakery Production Environment",
    "section": "Bellman’s Bakery (overview)",
    "text": "Bellman’s Bakery (overview)\nA cute pastel “Tiny Bakery Tycoon” reinforcement learning project. Each episode is one day at a small bakery with 2 ovens, limited capacity, 5 pastries, and real-time queues. Customers arrive with skewed preferences and varying patience. The agent plans baking and serving to maximize profit while minimizing waits, abandonment, and waste.\nWhat’s inside:\n\nMethods: PPO with action masking (deep RL) + small Thompson-sampling bandits for daily decisions (shallow RL).\nResults: Comparison vs heuristic baselines (bake-to-par, greedy-queue, newsvendor) and ablations (price bandit, par bandit).\nViewer: A minimal Pygame viewer for a cute visual of the day.\n\nRepo: bellmans_bakery environment and consolidated scripts/* runner in this project."
  }
]