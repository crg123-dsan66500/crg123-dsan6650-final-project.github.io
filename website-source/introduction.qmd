---
title: "Introduction"
format:
  html:
    toc: true
    embed-resources: true
bibliography: references.bib 
---

## Motivation

Operational choices in retail or service settings typically involve balancing multiple competing goals, such as increasing revenue, reducing customer wait times, preventing stockouts, and minimizing spoilage. Bakeries, for example, face short-term challenges with shelf-life items, limited space, and unpredictable demand that varies by hour. The well-known newsvendor model provides clear guidance for single-period ordering decisions, offering analytically optimal stocking levels under uncertain demand [@khouja1999newsvendor] [@arrow1951inventory]. However, these formulations focus primarily on a single objective and do not account for real-time dynamics such as delays, customer abandonment, congestion, or operational bottlenecks.

Deep reinforcement learning (DRL) has emerged as a useful method for developing control strategies through direct experience in complex settings. While these approaches can capture responses to shifting demand patterns, delays, or system limits, even without predefined models, they do not inherently aim to achieve goals like revenue or performance. DRL agents are only as good as the reward function they are trained on. They do not "optimize for profit" or "maximize customer satisfaction" unless those goals are explicitly and precisely defined. If the reward signal captures only part of what matters to a business, the resulting behavior can end up looking nothing like what a human decision-maker would actually want.

## Problem Formulation
Bellman’s Bakery is a custom OpenAI Gymnasium environment designed for this study to simulate a small bakery with capacity constraints, where each episode corresponds to one single day of operation. Customers arrive with stochastic demand and limited patience; they may leave at the sight of a long queue or if their wait times exceed a certain threshold. The agent(baker) decides which items to produce, when to serve customers, and when to idle, all while operating under constraints such as oven capacity and fixed bake times. At the end of each day, unsold inventory is discarded. The environment records business KPIs such as **net profit**, **average wait**, **abandonment**, and **waste**, and uses an action mask to enforce valid decisions. Its reward is a combined (weighted) signal: revenue and successful service add points, while waiting, abandonment, balking, and leftover inventory subtract points. This reward is intentionally not equivalent to net profit. The central question is how different solution approaches, **newsvendor-styleheuristics**,  **Proximal Policy Optimization (PPO)**, **PPO+bandits**, and **Quantile Regression Deep Q‑Network (QRDQN)**, optimize this shaped reward and how the resulting policies perform when judged against business-facing metrics such as profitability, service quality, and inventory efficiency.

To investigate this, four types of policies are benchmarked: 

- A **newsvendor-style heuristic** that selects bake quantities using a static demand model and a target service level (quantile rule).
- A **PPO agent** trained directly on the shaped reward in the Bellman’s Bakery environment.
- A **hybrid PPO+bandit variant** that augments PPO with a pricing or par-level bandit component.
- A **QRDQN agent**, representing an off-policy, value-based DRL method adapted to the same discrete action space.

This setup leads to the following research questions:

1. ***How does reward shaping in a multi-objective production system influence the qualitative behavior of deep RL agents?***
2. ***Under what conditions does a DRL policy outperform, match, or underperform a simple newsvendor heuristic when evaluated on profit, waiting time, abandonment, and waste?***
3. ***Do off-policy distributional methods such as QRDQN exhibit different trade-offs than on-policy methods like PPO under the same reward and environment?***

## Related Work
Despite substantial work at the intersection of perishable inventory, dynamic pricing, and reinforcement learning, most studies analyze settings that differ from the operational context examined in this paper. Research on perishable inventory typically focuses on optimal ordering and pricing strategies under stochastic demand, often assuming well-structured demand models and low-dimensional state spaces.For instance, Nomura and colleagues examined a perishable inventory issue involving dynamic pricing and demand that varies by age comparing traditional dynamic programming approaches to PPO methods [@deepRLforDynamicPricing]. In the end, they found that DRL could get close to the best possible results while simulataneously cutting down on the time needed for computations. Two conclusions are especially relevant here: (i) PPO performs well when the reward is tightly aligned with the economic objective, and (ii) reward design strongly shapes learned behavior.

Other lines of research examine RL in queueing and scheduling environments. Van Hezewijk shows that RL agents often prioritize surrogate reward terms, such as penalties for waiting or abandonment, over business-facing performance metrics when these are not tightly aligned [@5bc07a4907c842fe8910904e812a0b1f]. This raises concerns about whether RL agents may develop overly conservative or low-throughput policies in systems with capacity constraints. In parallel, dynamic pricing research has often employed bandit-based approaches rather than full RL. Genalti demonstrates that Thompson sampling can effectively learn profit-maximizing pricing strategies under uncertainty, especially when demand elasticity is monotonic and data are limited [@genalti2021dynamicpricing]. The pricing bandit used in this study reflects that lineage, though prior work suggests its effectiveness depends on whether pricing can meaningfully shift demand.

Taken together, these works indicate that RL methods are highly sensitive to reward specification and state–action structure, and that bandit-based pricing only adds value when price elasticity meaningfully shifts demand. Accordingly, this paper investigates three core questions in the context of a simulated, capacity-constrained bakery: (1) how PPO responds to a shaped reward that only partially reflects profit; (2) whether surrogate penalties for waiting and abandonment bias policies toward conservative, low-waste behavior; and (3) whether pricing flexibility improves performance when demand is uncertain but throughput is constrained by service capacity rather than elasticity.

## Contributions of this project
This project brings forward three key contributions. One involves a detailed queue-based simulation for a small bakery operation capturing those real tensions in daily work, like balancing profit against service quality and cutting down on waste. Another contribution comes from a structured comparison of various policy approaches through experiments that cover basic heuristics, on-policy reinforcement learning with PPO, hybrid bandit, RL methods, and an off-policy distributional technique called QRDQN. All of them faced evaluation under identical reward shaping. The third point reveals what happens when training rewards drift away from the actual evaluation measures. PPO starts to chase the wrong targets too hard. It pushes to minimize waste, even if that means sacrificing profit and service levels. Meanwhile, QRDQN along with heuristic policies land at varied spots on the same trade-off line. Overall, these outcomes point out something crucial: reward design and matching objectives really matter in deep reinforcement learning for production and inventory setups.